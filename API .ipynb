{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bibliothèque de base\n",
    "import spacy\n",
    "import fr_core_news_sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "#bibliothèque pour le traitement de texte\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "#bibliothèque pour la classification \n",
    "from scipy import linalg # SVD\n",
    "from sklearn import decomposition # NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chargement des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification du tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# je prends que les colonnes qui m'interresse\n",
    "col = ['category', 'text', 'content']\n",
    "df = df[col]\n",
    "df = df[pd.notnull(df['text'])]\n",
    "\n",
    "df.columns = ['category', 'text', 'content']\n",
    "\n",
    "# je factorise et crée des dictionnaires pour liér la catégorie à son id \n",
    "df['category_id'] = df['category'].factorize()[0]\n",
    "\n",
    "category_id_df = df[['category', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'category']].values)\n",
    "\n",
    "content_id_df = df[['category', 'content']].drop_duplicates().sort_values('content')\n",
    "content_to_id = dict(content_id_df.values)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement stopwords français\n",
    "stop_words = stopwords.words('french')\n",
    "    \n",
    "#Initialize stemmer, which will take words and convert words to their \"stem,\" e.g. Playing-> Play\n",
    "ps = PorterStemmer() \n",
    "\n",
    "# Enlever les charactères nons alphabétiques, espaces et convertir toutes les lettres en minuscules\n",
    "# References: https://www.analyticsvidhya.com/blog/2019/04/predicting-movie-genres-nlp-multi-label-classification/\n",
    "def clean_text(txt): \n",
    "    txt= txt.lower()   #lowercase\n",
    "    txt= re.sub(\"[^a-zA-Z]\",\" \",txt) #Remove everything except alphabetical characters \n",
    "    txt= word_tokenize(txt) #tokenize (split into list and remove whitespace)\n",
    "    \n",
    "    #initialize list to store clean text\n",
    "    clean_text=\"\"\n",
    "      \n",
    "    #iterate over each word\n",
    "    for w in txt:      \n",
    "        #remove stopwords\n",
    "        if w not in stop_words:\n",
    "            #stem=ps.stem(w) #stem \n",
    "            stem=w\n",
    "            clean_text += stem +\" \" \n",
    "    return clean_text\n",
    "\n",
    "\n",
    "text_new=[] #declare a list to hold new movies\n",
    "\n",
    "for cell in df['text']:    \n",
    "    txt= clean_text(cell)\n",
    "    text_new.append(txt)\n",
    "    \n",
    "#add new info column to the dataframe\n",
    "df['text'] = text_new \n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams des catégories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='utf-8', ngram_range=(1, 2), stop_words=final_stopwords_list)\n",
    "features = tfidf.fit_transform(df.text).toarray()\n",
    "labels = df.category_id\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## corrélation entre le thème et les mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "for category, category_id, in sorted(category_to_id.items()):\n",
    "    features_chi2 = chi2(features, labels == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}':\".format(category))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(features, labels)\n",
    "N = 2\n",
    "for Product, category_id in sorted(category_to_id.items()):\n",
    "  indices = np.argsort(model.coef_[category_id])\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n",
    "  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n",
    "  print(\"# '{}':\".format(Product))\n",
    "  print(\"  . Top unigrams:\\n       . {}\".format('\\n       . '.join(unigrams)))\n",
    "  print(\"  . Top bigrams:\\n       . {}\".format('\\n       . '.join(bigrams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['category'], random_state = 0)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# ignore ConvergenceWarnings\n",
    "from  warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "models = [\n",
    "    { \"estimater\": SGDClassifier(),\n",
    "      \"params\": {\n",
    "          \"loss\": ['hinge','log','perceptron'], \n",
    "          \"penalty\": ['l2', 'l1'], \n",
    "          \"alpha\": [ 0.0003],\n",
    "          \"early_stopping\": [True],\n",
    "          \"max_iter\": [1000],\n",
    "          \"random_state\": [42]\n",
    "      }\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.33, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=category_id_df.category.values, yticklabels=category_id_df.category.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tableau des métrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=df['category'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## affichage des erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "for predicted in category_id_df.category_id:\n",
    "    for actual in category_id_df.category_id:\n",
    "        if predicted != actual and conf_mat[actual, predicted] >= 5:\n",
    "            print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n",
    "            display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['category', 'text']])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graphique des métrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "df = pd.DataFrame({\"Estimater\": column_names, \n",
    "                   \"Accuracy\": entries\n",
    "                  })\n",
    "\n",
    "sns.barplot(x='Estimater', y='Accuracy', data=df)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
